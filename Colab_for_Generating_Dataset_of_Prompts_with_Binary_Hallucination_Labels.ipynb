{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "HtMGnNBiPHpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hnvcDMWIJcQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Manipulate Datasets"
      ],
      "metadata": {
        "id": "y-hpndoTPPXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset from https://github.com/RUCAIBox/HaluEval\n",
        "df1 = pd.read_json(\"/content/general_data.json\", lines=True)\n",
        "\n",
        "# data cleaning, only need prompt and binary yes/no for hallucination\n",
        "df1.drop(['chatgpt_response', 'ID', 'hallucination_spans'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "df1['hallucination_binary'] = df1['hallucination'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "df1.drop('hallucination', axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "# necessary for trainer\n",
        "df1.rename(columns={'hallucination_binary':'label'}, inplace=True)\n",
        "\n",
        "# rename for consistency\n",
        "df1.rename(columns={'user_query':'input'}, inplace=True)\n",
        "\n",
        "print(len(df1))\n",
        "print(df1.columns)\n",
        "print(df1.head())\n",
        "print(df1['label'].value_counts())"
      ],
      "metadata": {
        "id": "pPf392GZak_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc63ebbd-24de-4323-b900-0b154d7214d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4507\n",
            "Index(['input', 'label'], dtype='object')\n",
            "                                               input  label\n",
            "0  Produce a list of common words in the English ...      0\n",
            "1              Provide a few examples of homophones.      1\n",
            "2  Create a chart outlining the world's populatio...      1\n",
            "3         Design a shape with 10 vertices (corners).      1\n",
            "4  Automatically generate a 10 by 10 multiplicati...      1\n",
            "label\n",
            "0    3692\n",
            "1     815\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset includes generic questions from users such as the most common words in the English language. It contains 4507 samples and is heavily biased towards \"no hallucination,\" with 3692 prompts resulting in no hallucination and only 815 resulting in a hallucination in ChatGPT's response. To prepare this dataset, I converted the hallucination column to an integer label (0 for no hallucination, 1 for a hallucination) and removed all columns except for the prompt (input) and label."
      ],
      "metadata": {
        "id": "_Nh1ccJrjaVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets from https://github.com/Arize-ai/LibreEval\n",
        "csv_file_paths = [\n",
        "    '/content/docs_databricks_com_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/earthobservatory_nasa_gov_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/experienceleague_adobe_com_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/medlineplus_gov_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/pmc_ncbi_nlm_nih_gov_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/www_investopedia_com_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/www_law_cornell_edu_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/www_mongodb_com_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/www_ncbi_nlm_nih_gov_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    '/content/www_noaa_gov_gpt_4o_synthetic_gpt_4o_claude_3_5_sonnet_latest_en_answer.txt',\n",
        "    ]\n",
        "\n",
        "df2_list = [pd.read_csv(file) for file in csv_file_paths]\n",
        "df2 = pd.concat(df2_list, ignore_index=True)\n",
        "\n",
        "print(df2['label'].value_counts())\n",
        "\n",
        "df2.drop(columns=[\"reference\", \"output\", \"explanation_gpt-4o\",\n",
        "                  \"label_claude-3-5-sonnet-latest\", \"explanation_claude-3-5-sonnet-latest\",\n",
        "                  \"label_litellm/together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
        "                  \"explanation_litellm/together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
        "                  \"rag_model\", \"force_even_split\", \"website\", \"synthetic\",\n",
        "                  \"language\", \"hallucination_type_realized\", \"question_type\", \"hallucination_type_encouraged\",\n",
        "                  \"hallucination_type_realized_ensemble\", \"label_mistral-large-latest\",\n",
        "                  \"explanation_mistral-large-latest\", \"human_label\", \"label_gpt-4o\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "df2 = df2[df2['label'].str.upper() != \"NOT_PARSABLE\"]\n",
        "df2['label'] = df2['label'].map({'hallucinated': 1, 'factual': 0})\n",
        "\n",
        "print(len(df2))\n",
        "print(df2.columns)\n",
        "print(df2.head())\n",
        "print(df2['label'].value_counts())"
      ],
      "metadata": {
        "id": "FzI7NyluJbKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ddf5f7-5470-4456-c9d4-71ca98de7f3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "factual         3270\n",
            "hallucinated     976\n",
            "NOT_PARSABLE       2\n",
            "Name: count, dtype: int64\n",
            "4246\n",
            "Index(['input', 'label'], dtype='object')\n",
            "                                               input  label\n",
            "0  What actions can be performed on an external l...      0\n",
            "1  What versions of Databricks Runtime does the i...      0\n",
            "2  What is the default access restriction for mat...      0\n",
            "3  Who can query materialized views and streaming...      0\n",
            "4  What is required to enable Iceberg reads on ta...      0\n",
            "label\n",
            "0    3270\n",
            "1     976\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used LibreEval's synthetic \"even-split-of-hallucinations-and-factuals\" datasets (path: labeled_datasets/gpt-4o-hallucinations/synthetic/even-split-of-hallucinations-and-factuals) in the hopes of reducing the dataset imbalance caused by HaluEval, but even before my own manipulation there was an uneven distribution of factual and hallucinated examples--3270 samples were factual, and 976 hallucinated (77% to 23%).\n",
        "\n",
        "I used the \"answer\" datasets for ChatGPT to retreive both the prompt and whether or not a hallucination occured, converted the label column to an integer value (0 for no hallucination, 1 for a hallucination).\n",
        "\n",
        "Here are descriptions for a few of the datasets which I used:\n",
        "The databricks docs answer dataset contains user queries about the [databricks](https://www.databricks.com/) docs, such as how comments are handled in databricks. The earth observatory dataset contains user queries about the environment and science such as how snow and ice influence the climate from [their data](https://science.nasa.gov/earth/earth-observatory/).\n",
        "\n",
        "Overall, these datasets combine a wide range of topics for a total of 4246 data points of prompts paired with whether or not ChatGPT hallucinated. In order to answer correctly, the LLM must extract the answer from the corresponding webpage. If you want to check out more of these datasets I used, you can see them [here](https://github.com/Arize-ai/LibreEval/tree/main/labeled_datasets/gpt-4o-hallucinations/synthetic/even-split-of-hallucinations-and-factuals/en)."
      ],
      "metadata": {
        "id": "MHt6rKwQkiQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset from https://huggingface.co/datasets/opencompass/anah?row=0\n",
        "from datasets import load_dataset\n",
        "\n",
        "anah_dataset = load_dataset(\"opencompass/anah\")\n",
        "\n",
        "# only has train set on hugging face\n",
        "df3 = anah_dataset[\"train\"].to_pandas()\n",
        "\n",
        "# filter for english\n",
        "df3 = df3[df3['language'] == 'en']\n",
        "\n",
        "# drop non-gpt columns\n",
        "df3.drop(columns=[\"InternLM_answers\", \"human_InternLM_answers_ann\", \"name\", \"documents\", \"language\", \"GPT3.5_answers_D\"], inplace=True)\n",
        "\n",
        "# get true/false hallucination data, code from ChatGPT\n",
        "df3['label'] = df3['human_GPT3.5_answers_D_ann'].apply(\n",
        "    lambda ann: any('<Hallucination>' in str(a) for a in ann)\n",
        ")\n",
        "\n",
        "df3['label'] = df3['label'].map({True: 1, False: 0})\n",
        "\n",
        "# remove LLM response\n",
        "df3.drop(columns=\"human_GPT3.5_answers_D_ann\", inplace=True)\n",
        "\n",
        "# rename for consistency\n",
        "df3.rename(columns={'selected_questions':'input'}, inplace=True)\n",
        "\n",
        "# extract strings for arrays in every row, code from ChatGPT\n",
        "\n",
        "df3['input'] = df3['input'].apply(\n",
        "    lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else x\n",
        ")\n",
        "\n",
        "print(df3.head())\n",
        "print(df3.columns)\n",
        "print(len(df3))\n",
        "print(df3['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouiiV-sALRr3",
        "outputId": "cb10144b-f878-42e3-f84a-af3234b48bd5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               input  label\n",
            "0  What was the aftermath of the Battle of Sobrao...      1\n",
            "1  What were the consequences of the Kapp Putsch ...      1\n",
            "2  What were the main factors leading to the Batt...      0\n",
            "3  How did the Battle of the Camel unfold, and wh...      0\n",
            "4  How was the leadership vote conducted and what...      1\n",
            "Index(['input', 'label'], dtype='object')\n",
            "497\n",
            "label\n",
            "1    454\n",
            "0     43\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANAH contains a comprehensive range of 700 topics and its 1.2k questions are primarily factual/historical in nature. It contains 497 data points in English, which I filtered for. I then used ChatGPT to write code which would take the answer analysis column (human analysis of ChatGPT answers) and convert it to either 0 or 1 in a label column depending on whether or not it contained \"&lt;Hallucination&gt;\". This works because responses which contain hallucinations have \"&lt;Hallucination&gt;\". in their breakdown while those which do not have an empty analysis. Finally, I dropped all columns which weren't the original input or the derived label.\n",
        "\n",
        "Here's the citation to this dataset:\n",
        "\n",
        "```\n",
        "@inproceedings{ji2024anah,\n",
        "  title={ANAH: Analytical Annotation of Hallucinations in Large Language Models},\n",
        "  author={Ji, Ziwei and Gu, Yuzhe and Zhang, Wenwei and Lyu, Chengqi and Lin, Dahua and Chen, Kai},\n",
        "  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n",
        "  pages={8135--8158},\n",
        "  year={2024}\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7ROSoJal89Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Datasets"
      ],
      "metadata": {
        "id": "UnuijORkPTeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "print(combined_df.shape)\n",
        "print(combined_df.columns)\n",
        "print(combined_df.head())\n",
        "\n",
        "print(combined_df['label'].value_counts())\n",
        "\n",
        "\n",
        "combined_df.to_csv(\"final_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "cJBe95NFJSXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382e0938-d116-4385-edda-28756fde1606"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9250, 2)\n",
            "Index(['input', 'label'], dtype='object')\n",
            "                                               input  label\n",
            "0  Produce a list of common words in the English ...      0\n",
            "1              Provide a few examples of homophones.      1\n",
            "2  Create a chart outlining the world's populatio...      1\n",
            "3         Design a shape with 10 vertices (corners).      1\n",
            "4  Automatically generate a 10 by 10 multiplicati...      1\n",
            "label\n",
            "0    7005\n",
            "1    2245\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My final dataset has 9250 data points. It has 7005 examples which didn't cause a hallucination (75.73%) and 2245 which did (24.27%). The final dataset contains two columns: an \"input\", representing a user's prompt to ChatGPT, and a \"label\", an integer value (0 or 1) representing whether or not that prompt caused a hallucination. If you have run each of these cells (which requires downloading the original datasets) you can download the final dataset as a .csv from Colab's file display. If not, it can be downloaded directly from my GitHub."
      ],
      "metadata": {
        "id": "ddxy-pFpPF_C"
      }
    }
  ]
}